---
title: "Data Analysis and Modeling for Next Word Prediction App Production"
author: "kosw"
date: "2024-02-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, cache=TRUE, fig.height=4, fig.width=10)
```

## Synopsis
In this report, we deal with plan for producing application that recommends the most likely next word.
For this, with dataset from swiftkey, exploratory analysis was conducted. After it, prediction model was constructed
combining trigram model with Katz's backoff model that is for dealing with zero probability problem.
Based on it, we described application that presents the 3 most likely words to users.



## About Dataset
Dataset we used in this report are from 'swiftkey', contain corpora obtained from blogs, news, twitters in each of 4 languages(German, English,
Finnish, Russian). Among them, we used English dataset. An example of English news document is as follows.

'The St. Louis plant had to close. It would die of old age. Workers had been making cars there since the onset of mass automotive production in the 1920s.'


## Exploratory Data Analysis
In 3 files(blogs, news, twitters), total number of documents(lines) is following.

```{r, echo=FALSE, cache=TRUE, warning=FALSE}
blog <- readLines("./final/en_US/en_US.blogs.txt")
news <- readLines("./final/en_US/en_US.news.txt")
twitter <- readLines("./final/en_US/en_US.twitter.txt")
names <- c('blog', 'news', 'twitter')
lengths <- c(length(blog), length(news), length(twitter))


for(i in 1:3){
  print(paste(paste(names[i], ':'), lengths[i]))
}
```




And When we sample 1000 lines from each files, total frequency of unigrams(words) is
```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
packages <- c('LaF', 'stringr', 'sentimentr', 'textstem', 'lexicon')
for(package in packages){
  if(!(package %in% installed.packages())){
    install.packages(package)
  }
  library(package, character.only = TRUE)
}

set.seed(123)
data1 <- sample_lines("./final/en_US/en_US.blogs.txt", n=1000)
data2 <- sample_lines("./final/en_US/en_US.news.txt", n=1000)
data3 <- sample_lines("./final/en_US/en_US.twitter.txt", n=1000)
datas <- list(data1=data1, data2=data2, data3=data3)
names <- c('blog', 'news', 'twitter')
total_freq <- c()
uni_blog <- c()
uni_news <- c()
uni_twitter <- c()
uni_list <- list(uni_blog=uni_blog, uni_news=uni_news, uni_twitter=uni_twitter)

for(i in 1:length(datas)){
  data <- datas[[i]]
  data <- str_remove(data, pattern="\\r$")
  data <- str_split(data, "\\s")
  
  data <- lapply(data, tolower)
  data <- lapply(data, lemmatize_words)
  
  data <- lapply(data, str_split, pattern="(?=[:punct:])")
  data <- lapply(data, unlist)
  data <- lapply(data, str_split, pattern="(?<=[:punct:])")
  data <- lapply(data, unlist)
  data <- lapply(data, str_split, pattern="(?=[$])")
  data <- lapply(data, unlist)
  data <- lapply(data, str_split, pattern="(?<=[$])")
  data <- lapply(data, unlist)
  data <- lapply(data, function(x) return(x[x !=""]))
  
  data <- lapply(data, str_split, pattern="(?=[0-9])")
  data <- lapply(data, unlist)
  data <- lapply(data, str_split, pattern="(?<=[0-9])")
  data <- lapply(data, unlist)
  data <- lapply(data, function(x) return(x[x !=""]))
  
  whether_punct <- lapply(data, grepl, pattern="([[:punct:]])")
  whether_punct <- lapply(whether_punct, function(x) x = !x)

  for(ii in 1:length(data)){
    data[[ii]] <- data[[ii]][whether_punct[[ii]]]
  }
  
  #removing numbers
  whether_num <- lapply(data, grepl, pattern="([0-9])")
  whether_num <- lapply(whether_num, function(x) x = !x)
  
  for(iii in 1:length(data)){
    data[[iii]] <- data[[iii]][whether_num[[iii]]]
  }
  
  


  pro1 <- lexicon::profanity_alvarez
  pro2 <- lexicon::profanity_banned
  pro3 <- lexicon::profanity_arr_bad
  pro4 <- lexicon::profanity_racist
  pro5 <- lexicon::profanity_zac_anger

  profanity <- union(union(union(pro1, pro2), union(pro3, pro4)), pro5)

  #Profanity counting
  firstcounting <- sum(profanity(unlist(data), profanity)$profanity_count)

  #Removing profanity
  remove_profanity <- function(text, profanity){
    for(word in profanity){
      matching <- grep(pattern=word, x=text, fixed = TRUE)
      if(length(matching) >= 1){
        text <- text[-matching]
      }
    }
    return(text)
  }

  data <- lapply(data, remove_profanity, profanity = profanity)


  #Profanity 'Re'counting
  recounting <- sum(profanity(unlist(data), profanity)$profanity_count)

  #profanity that wasn't removed finding and adding it to lexicon and remove again with new lexicon
  if(recounting != 0){
    profanity_index <- which(profanity(unlist(data), profanity)$profanity_count != 0)
    unremoved_profanity <- unlist(data)[profanity_index]
    unremoved_profanity
    profanity[length(profanity)+seq(from=1, by=1, length.out=length(unremoved_profanity))] <- unremoved_profanity

    data <- lapply(data, remove_profanity, profanity = profanity)
  }

  
  data <- lapply(data, lemmatize_words)

  total_freq[i] <- length(unlist(data))
  
  uni_list[[i]] <- data
}

for(t in 1:3){
  print(paste(paste(names[t], ":"), total_freq[t]))
}

print(paste('total :', sum(total_freq)))

```
And when we sample 1000 lines in each of 3 files, the size of vocabulary in each file and total is
```{r}
for(as in 1:length(uni_list)){
  x <- unlist(uni_list[[as]])
  print(paste(paste(names[as], ":"), length(unique(x))))
}

y <- unlist(uni_list)
print(paste('total :', length(unique(y))))
```
So, how many words in vocabulary make up most of the frequency?
```{r}
unigram_freq <- matrix(ncol=2)
unigram_freq <- as.data.frame(unigram_freq)
colnames(unigram_freq) <- c('word', 'freq')
num <- 1

for(unigram in unique(unlist(uni_list))){
  unigram_freq[num,1] <- unigram
  unigram_freq[num,2] <- sum(unlist(uni_list)==unigram)
  num <- num + 1
}

unigram_freq <- unigram_freq[order(unigram_freq[,2], decreasing = TRUE),]

for(aw in 1:dim(unigram_freq)[1]){
  unigram_freq[aw,3] <- 100*sum(unigram_freq[1:aw,2])/length(unlist(uni_list))
}

colnames(unigram_freq)[3] <- 'cumul'

length(unigram_freq[,1])

percents <- c(50, 60, 70, 80, 90)
for(percent in percents){
  print(paste(paste(paste(paste(paste(which(unigram_freq[,3] >= percent)[1],'words among'), length(unigram_freq[,1])),  ' words make up'), percent),'% of the total frequency'))
}

```


This result suggests chance to improvement of runtime efficiency by reducing the size of vocabulary.


The distributions of the number of tokens in each lines in each of 3 files is following.
```{r, echo=FALSE, cache=TRUE}
num_token <- list()

for(q in 1:length(uni_list)){
  num_token[[q]] <- lapply(uni_list[[q]], length)
}


par(mfrow=c(1, 3))
for(r in 1:length(num_token)){
  hist(unlist(num_token[[r]]), main=paste("Number of tokens in", names[r]), xlab = 'Number of tokens',
       breaks = unlist(num_token[[1]])[which.max(unlist(num_token[[r]]))])
}

```
<br/>
And maximum length of line is
```{r, echo=FALSE, cache=TRUE}
for(s in 1:length(uni_list)){
  print(paste(paste(names[s], ":"), unlist(num_token[[s]])[which.max(unlist(num_token[[s]]))]))
}

```

The most frequent unigrams in each of 3 files is
```{r, echo=FALSE, cache=TRUE}
#Make word frequency data
word_frequency <- matrix(ncol=9)
word_frequency <- as.data.frame(word_frequency)

for(u in 1:length(uni_list)){
  num <- 1
  for(word in unique(unlist(uni_list[[u]]))){
    word_frequency[num, 3*(u-1)+1] <- names[u]
    word_frequency[num, 3*(u-1)+2] <- word
    word_frequency[num, 3*(u-1)+3] <- sum(unlist(uni_list[[u]])==word)
    num <- num + 1
  }
}


par(mfrow=c(1, 3))
for(t in 1:length(uni_list)){
  word_frequency <- word_frequency[order(word_frequency[,3*t],decreasing = TRUE),]
  barplot(head(word_frequency[,3*t],10), names.arg = head(word_frequency[,3*t-1],10),
          main=paste('Top 10 word freuency in', names[t]), cex.names = 0.8)
}
```
<br/>
We can see that most of frequent words are stopwords(e.g.,'the', 'a').
 So, let's check the frequency again without stopwords!

```{r, echo=FALSE, cache=TRUE}
if(!('stopwords' %in% installed.packages())){
  install.packages('stopwords')
}
library(stopwords)
stopwords <- c(stopwords(), "re", "d", "ll", "t", "s", "m", "ve", "us", "don", "st", "th", "u", "didn")
stopwords <- stopwords[-grep(pattern="\'", stopwords)]

uni_list1 <- uni_list

remove_stopword <- function(text, stopword){
  for(word in stopword){
    matching <- grep(pattern=paste("^", word, "$", sep=""), x=text)
    if(length(matching) >= 1){
      text <- text[-matching]
    }
  }
  return(text)
}

for(v in 1:length(uni_list1)){
  uni_list1[[v]] <- lapply(uni_list1[[v]], remove_stopword, stopword = stopwords)
}


#Make word frequency data
word_frequency1 <- matrix(ncol=9)
word_frequency1 <- as.data.frame(word_frequency1)

for(w in 1:length(uni_list1)){
  num <- 1
  for(word in unique(unlist(uni_list1[[w]]))){
    word_frequency1[num, 3*(w-1)+1] <- names[w]
    word_frequency1[num, 3*(w-1)+2] <- word
    word_frequency1[num, 3*(w-1)+3] <- sum(unlist(uni_list1[[w]])==word)
    num <- num + 1
  }
}


par(mfrow=c(1, 3))
for(x in 1:length(uni_list)){
  word_frequency1 <- word_frequency1[order(word_frequency1[,3*x],decreasing = TRUE),]
  barplot(head(word_frequency1[,3*x],10), names.arg = head(word_frequency1[,3*x-1],10),
          main=paste('Top 10 non-stopword freuency in', names[x]), cex.names = 0.6)
}
```
<br/>
And let's check the frequency of bigrams!
```{r, echo=FALSE, cache=TRUE}
make_bigrams <- function(text){
  tokens <- c()
  for(i in 1:length(text)-1){
    tokens[i] <- paste(text[i], text[i+1])
  }
  tokens
}

bi_blog <- c()
bi_news <- c()
bi_twitter <- c()
bi_list <- list(bi_blog=bi_blog, bi_news=bi_news, bi_twitter=bi_twitter)

for(aa in 1:length(uni_list)){
  bi_list[[aa]] <- lapply(uni_list[[aa]], make_bigrams)
}

#Make bigram frequency data
bi_frequency <- matrix(ncol=9)
bi_frequency <- as.data.frame(bi_frequency)

for(ab in 1:length(bi_list)){
  num <- 1
  for(bigram in unique(unlist(bi_list[[ab]]))){
    bi_frequency[num, 3*(ab-1)+1] <- names[ab]
    bi_frequency[num, 3*(ab-1)+2] <- bigram
    bi_frequency[num, 3*(ab-1)+3] <- sum(unlist(bi_list[[ab]])==bigram)
    num <- num + 1
  }
}


par(mfrow=c(1, 3))
for(ac in 1:length(bi_list)){
  bi_frequency <- bi_frequency[order(bi_frequency[,3*ac],decreasing = TRUE),]
  barplot(head(bi_frequency[,3*ac],5), names.arg = head(bi_frequency[,3*ac-1],5),
          main=paste('Top 5 bigram frequency in', names[ac]), cex.names = 1)
}
```
<br/>
Also check in trigram.
<br/>
```{r, echo=FALSE, cache=TRUE}
make_trigrams <- function(text){
  tokens <- c()
  for(i in 1:length(text)-2){
    tokens[i] <- paste(text[i], text[i+1], text[i+2])
  }
  tokens
}

tri_blog <- c()
tri_news <- c()
tri_twitter <- c()
tri_list <- list(tri_blog=tri_blog, tri_news=tri_news, tri_twitter=tri_twitter)

for(ad in 1:length(uni_list)){
  tri_list[[ad]] <- lapply(uni_list[[ad]], make_trigrams)
}

#Make trigram frequency data
tri_frequency <- matrix(ncol=9)
tri_frequency <- as.data.frame(tri_frequency)

for(ae in 1:length(tri_list)){
  num <- 1
  for(trigram in unique(unlist(tri_list[[ae]]))){
    tri_frequency[num, 3*(ae-1)+1] <- names[ae]
    tri_frequency[num, 3*(ae-1)+2] <- trigram
    tri_frequency[num, 3*(ae-1)+3] <- sum(unlist(tri_list[[ae]])==trigram)
    num <- num + 1
  }
}


par(mfrow=c(1, 3))
for(af in 1:length(tri_list)){
  tri_frequency <- tri_frequency[order(tri_frequency[,3*af],decreasing = TRUE),]
  barplot(head(tri_frequency[,3*af],5), names.arg = head(tri_frequency[,3*af-1],5),
          main=paste('Top 5 trigram frequency in', names[af]), cex.names = 0.8)
}
```


## Prediction Model
### Trigram(3-gram) model
'n-gram model' is model assuming that the probability of nth word depends not on all of the previous words, but on previous (n-1) words.
For example, there are 5 words, 'a', 'b', 'c', 'd', 'e', and document 'abcde', the probability that 'e' occurs after 'abcd' can be expressed as 
<br/>
'P(e|abcd) = P(e|cd)'. 
<br/>
Also, probability that 'd' occurs after 'abc' can be expressed as 'P(d|bc)'.

Among various n-gram model according to the value of n, we will use 'trigram(3-gram) model' for app production. So we will calculate the probability that the next word is 'w3' using previous 2 words(w1w2).
<br/>
### Katz's Backoff Model
Even though there are a massive kind of n-grams that can occur in real world, there is a problem that zero probability is given to n-grams that have not seen in training set. To deal with this problem, we will use 'Katz's backoff model' along with the trigram model. This model estimate the probability of unseen n-grams by distributing the probability mass taken from seen n-grams in training set. The specific method of estimation is following.

<br/>
When there is a 3-gram, 'w1w2'w3, where 'w1w2' is determined, we can divide the number of cases into 3 in according to whether the any 'w1w2w3' exists in the training set.


When 'w1w2' is determined,(e.g. 'ab')
<br/>
1. Case that any kind of 'w1w2w3' exists in the training set.(in form of 'abc')
<br/>
2. Case that any kind of 'w1w2w3' does not exist, but 'w2w3' exists.(in form of 'dbc')
<br/>
3. Case that even 'w2w3' does not exist.(in form of 'dec')

In each case, probability Ps(w3|w1w2) is calculated as follows.(Ps() means 'estimated probability')


*Ps(A|B) = d x P(A|B)
<br/>
P(A|B) = c(A&B)/c(B)
(where c(k) is frequency of k in training set.)
<br/>
<br/>
1. Case that any kind of 'w1w2w3' exists in the training set.(in form of 'abc')

1-1) About 'w3' that 'w1w2w3' exists when 'w1w2' is determined(any 'c' of 'abc')
<br/>
Ps(w3|w1w2) = d x P(w3|w1w2)
<br/>
Multipying by constant is for taking some probability mass from 'w1w2w3' existing in training set. That is called 'discounting', and constant 'd' is refered to as 'discounting rate'

1-2) About 'w3' in the rest of vocabulary that 'w1w2w3' does not exist, but 'w2w3' exist with other 'w1'(any 'c' of 'dbc')
<br/>
Ps(w3|w1w2) = a x Ps(w3|w2)
<br/>
It means redistribution of probability mass taken from 1-1) to 'w3' in case 1-2) in according to proportion of 'Ps(w3|w2)'
<br/><br/>
2. Case that any kind of 'w1w2w3' does not exist, but 'w2w3' exists.(in form of 'dbc')

2-1) About 'w3' that have 'w2w3'(in form of 'dbc')
<br/>
Ps(w3|w1w2) is estimated to be Ps(w3|w2)

2-2) About 'w3' in the rest of vocabulary that does not have 'w2w3'(in form of 'dec')
<br/>
Ps(w3|w1w2) = a x Ps(w3)
<br/>
It also means redistribution of probability mass taken from 2-1) to 'w3' in case 2-2) in according to proportion of 'Ps(w3|w2)'
<br/><br/>
3. Case that even 'w2w3' does not exist.(in form of 'dec')
<br/>
Ps(w3|w1w2) is estimated to be Ps(w3).

<br/>
In conclusion, by combining 'trigram model' and 'Katz's backoff model', the next word is predicted by estimating the probability of seen and unseen 3-grams when the previous 2 word is given.




## About Application
Our application presents the most likely word as next word based on previous two words while users are typing. Specific operation process is following.

1. The user types the text.
2. Texts typed are preprocessed so that algorithm can recognize them.
3. The 3 most likely words are predicted and presented to user based on previous 2 words.
4. If user select predicted word, the word is typed on the screen automatically.
















