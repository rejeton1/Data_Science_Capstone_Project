---
title: "Data Analysis and Modeling for Next Word Prediction App Production"
author: "kosw"
date: "2024-02-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis
In this report, we deal with plan for producing application that recommends the most likely next word.
For this, with dataset from swiftkey, exploratory analysis was conducted. After it, prediction model was constructed
combining trigram model with Katz's backoff model that is for dealing with zero probability problem.
Based on it, we described application that presents the 3 most likely words to users.



## About Dataset
Dataset we used in this report are from 'swiftkey', contain corpora obtained from blogs, news, twitters in each of 4 languages(German, English,
Finnish, Russian). Among them, we used English dataset. The example of English news document is as follows.

'The St. Louis plant had to close. It would die of old age. Workers had been making cars there since the onset of mass automotive production in the 1920s.'


## Exploratory Data Analysis
About 3 files(blogs, news, twitters), total number of documents(lines) is following.

```{r, echo=FALSE, cache=TRUE, warning=FALSE}
blog <- readLines("./final/en_US/en_US.blogs.txt")
news <- readLines("./final/en_US/en_US.news.txt")
twitter <- readLines("./final/en_US/en_US.twitter.txt")
names <- c('blog', 'news', 'twitter')
lengths <- c(length(blog), length(news), length(twitter))


for(i in 1:3){
  print(paste(paste(names[i], ':'), lengths[i]))
}
```




And When we sample 1000 lines from each files, total frequency of unigrams(words) is
```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
packages <- c('LaF', 'stringr', 'sentimentr', 'textstem', 'lexicon')
for(package in packages){
  if(!(package %in% installed.packages())){
    install.packages(package)
  }
  library(package, character.only = TRUE)
}

set.seed(123)
data1 <- sample_lines("./final/en_US/en_US.blogs.txt", n=1000)
data2 <- sample_lines("./final/en_US/en_US.news.txt", n=1000)
data3 <- sample_lines("./final/en_US/en_US.twitter.txt", n=1000)
datas <- list(data1=data1, data2=data2, data3=data3)
names <- c('blog', 'news', 'twitter')
total_freq <- c()
uni_blog <- c()
uni_news <- c()
uni_twitter <- c()
uni_list <- list(uni_blog=uni_blog, uni_news=uni_news, uni_twitter=uni_twitter)

for(i in 1:length(datas)){
  data <- datas[[i]]
  data <- str_remove(data, pattern="\\r$")
  data <- str_split(data, "\\s")
  
  data <- lapply(data, str_split, pattern="(?=[:punct:])")
  data <- lapply(data, unlist)
  data <- lapply(data, str_split, pattern="(?<=[:punct:])")
  data <- lapply(data, unlist)
  data <- lapply(data, str_split, pattern="(?=[$])")
  data <- lapply(data, unlist)
  data <- lapply(data, str_split, pattern="(?<=[$])")
  data <- lapply(data, unlist)
  data <- lapply(data, function(x) return(x[x !=""]))
  
  data <- lapply(data, str_split, pattern="(?=[0-9])")
  data <- lapply(data, unlist)
  data <- lapply(data, str_split, pattern="(?<=[0-9])")
  data <- lapply(data, unlist)
  data <- lapply(data, function(x) return(x[x !=""]))
  
  whether_punct <- lapply(data, grepl, pattern="([[:punct:]])")
  whether_punct <- lapply(whether_punct, function(x) x = !x)

  for(ii in 1:length(data)){
    data[[ii]] <- data[[ii]][whether_punct[[ii]]]
  }
  
  #removing numbers
  whether_num <- lapply(data, grepl, pattern="([0-9])")
  whether_num <- lapply(whether_num, function(x) x = !x)
  
  for(iii in 1:length(data)){
    data[[iii]] <- data[[iii]][whether_num[[iii]]]
  }
  
  data <- lapply(data, tolower)


  pro1 <- lexicon::profanity_alvarez
  pro2 <- lexicon::profanity_banned
  pro3 <- lexicon::profanity_arr_bad
  pro4 <- lexicon::profanity_racist
  pro5 <- lexicon::profanity_zac_anger

  profanity <- union(union(union(pro1, pro2), union(pro3, pro4)), pro5)

  #Profanity counting
  firstcounting <- sum(profanity(unlist(data), profanity)$profanity_count)

  #Removing profanity
  remove_profanity <- function(text, profanity){
    for(word in profanity){
      matching <- grep(pattern=word, x=text, fixed = TRUE)
      if(length(matching) >= 1){
        text <- text[-matching]
      }
    }
    return(text)
  }

  data <- lapply(data, remove_profanity, profanity = profanity)


  #Profanity 'Re'counting
  recounting <- sum(profanity(unlist(data), profanity)$profanity_count)

  #profanity that wasn't removed finding and adding it to lexicon and remove again with new lexicon
  if(recounting != 0){
    profanity_index <- which(profanity(unlist(data), profanity)$profanity_count != 0)
    unremoved_profanity <- unlist(data)[profanity_index]
    unremoved_profanity
    profanity[length(profanity)+seq(from=1, by=1, length.out=length(unremoved_profanity))] <- unremoved_profanity

    data <- lapply(data, remove_profanity, profanity = profanity)
  }

  data <- lapply(data, lemmatize_words)
  

  total_freq[i] <- length(unlist(data))
  
  uni_list[[i]] <- data
}

for(t in 1:3){
  print(paste(paste(names[t], ":"), total_freq[t]))
}

```

The distributions of the number of tokens in each lines in each of 3 files is following.
```{r, echo=FALSE, cache=TRUE}
num_token <- list()

for(q in 1:length(uni_list)){
  num_token[[q]] <- lapply(uni_list[[q]], length)
}

par(mfrow=c(1, 3))
for(r in 1:length(num_token)){
  hist(unlist(num_token[[r]]), main=paste("Number of tokens in", names[r]), xlab = 'Number of tokens',
       breaks = unlist(num_token[[1]])[which.max(unlist(num_token[[r]]))])
}

```
And maximum length of line is
```{r, echo=FALSE, cache=TRUE}
for(s in 1:length(uni_list)){
  print(paste(paste(names[s], ":"), unlist(num_token[[s]])[which.max(unlist(num_token[[s]]))]))
}

```

The most frequent unigrams in each of 3 files is
```{r, echo=FALSE, cache=TRUE}
#Make word frequency data
word_frequency <- matrix(ncol=9)
word_frequency <- as.data.frame(word_frequency)

for(u in 1:length(uni_list)){
  num <- 1
  for(word in unique(unlist(uni_list[[u]]))){
    word_frequency[num, 3*(u-1)+1] <- names[u]
    word_frequency[num, 3*(u-1)+2] <- word
    word_frequency[num, 3*(u-1)+3] <- sum(unlist(uni_list[[u]])==word)
    num <- num + 1
  }
}


par(mfrow=c(1, 3))
for(t in 1:length(uni_list)){
  word_frequency <- word_frequency[order(word_frequency[,3*t],decreasing = TRUE),]
  barplot(head(word_frequency[,3*t],10), names.arg = head(word_frequency[,3*t-1],10),
          main=paste('Top 10 word freuency in', names[t]), cex.names = 0.6)
}
```
We can see that most of frequent words are stopwords(e.g.,'the', 'a')
So, let's check the frequency again without stopwords!

```{r, echo=FALSE, cache=TRUE}
if(!('stopwords' %in% installed.packages())){
  install.packages('stopwords')
}
library(stopwords)
stopwords <- c(stopwords(), "re", "d", "ll", "t", "s", "m", "ve", "us", "don", "st", "th", "u", "didn")
stopwords <- stopwords[-grep(pattern="\'", stopwords)]

uni_list1 <- uni_list

remove_stopword <- function(text, stopword){
  for(word in stopword){
    matching <- grep(pattern=paste("^", word, "$", sep=""), x=text)
    if(length(matching) >= 1){
      text <- text[-matching]
    }
  }
  return(text)
}

for(v in 1:length(uni_list1)){
  uni_list1[[v]] <- lapply(uni_list1[[v]], remove_stopword, stopword = stopwords)
}


#Make word frequency data
word_frequency1 <- matrix(ncol=9)
word_frequency1 <- as.data.frame(word_frequency1)

for(w in 1:length(uni_list1)){
  num <- 1
  for(word in unique(unlist(uni_list1[[w]]))){
    word_frequency1[num, 3*(w-1)+1] <- names[w]
    word_frequency1[num, 3*(w-1)+2] <- word
    word_frequency1[num, 3*(w-1)+3] <- sum(unlist(uni_list1[[w]])==word)
    num <- num + 1
  }
}


par(mfrow=c(1, 3))
for(x in 1:length(uni_list)){
  word_frequency1 <- word_frequency1[order(word_frequency1[,3*x],decreasing = TRUE),]
  barplot(head(word_frequency1[,3*x],10), names.arg = head(word_frequency1[,3*x-1],10),
          main=paste('Top 10 word freuency in', names[t]), cex.names = 1)
}
```
And let's check the frequency of bigrams!
```{r, echo=FALSE, cache=TRUE}
make_bigrams <- function(text){
  tokens <- c()
  for(i in 1:length(text)-1){
    tokens[i] <- paste(text[i], text[i+1])
  }
  tokens
}

bi_blog <- c()
bi_news <- c()
bi_twitter <- c()
bi_list <- list(bi_blog=bi_blog, bi_news=bi_news, bi_twitter=bi_twitter)

for(aa in 1:length(uni_list)){
  bi_list[[aa]] <- lapply(uni_list[[aa]], make_bigrams)
}

#Make bigram frequency data
bi_frequency <- matrix(ncol=9)
bi_frequency <- as.data.frame(bi_frequency)

for(ab in 1:length(bi_list)){
  num <- 1
  for(bigram in unique(unlist(bi_list[[ab]]))){
    bi_frequency[num, 3*(ab-1)+1] <- names[ab]
    bi_frequency[num, 3*(ab-1)+2] <- bigram
    bi_frequency[num, 3*(ab-1)+3] <- sum(unlist(bi_list[[ab]])==bigram)
    num <- num + 1
  }
}


par(mfrow=c(1, 3))
for(ac in 1:length(bi_list)){
  bi_frequency <- bi_frequency[order(bi_frequency[,3*ac],decreasing = TRUE),]
  barplot(head(bi_frequency[,3*ac],5), names.arg = head(bi_frequency[,3*ac-1],5),
          main=paste('Top 5 bigram frequency in', names[ac]), cex.names = 1)
}
```
Also check in trigram.
```{r}
make_trigrams <- function(text){
  tokens <- c()
  for(i in 1:length(text)-2){
    tokens[i] <- paste(text[i], text[i+1], text[i+2])
  }
  tokens
}

tri_blog <- c()
tri_news <- c()
tri_twitter <- c()
tri_list <- list(tri_blog=tri_blog, tri_news=tri_news, tri_twitter=tri_twitter)

for(ad in 1:length(uni_list)){
  tri_list[[ad]] <- lapply(uni_list[[ad]], make_trigrams)
}

#Make trigram frequency data
tri_frequency <- matrix(ncol=9)
tri_frequency <- as.data.frame(tri_frequency)

for(ae in 1:length(tri_list)){
  num <- 1
  for(trigram in unique(unlist(tri_list[[ae]]))){
    tri_frequency[num, 3*(ae-1)+1] <- names[ae]
    tri_frequency[num, 3*(ae-1)+2] <- trigram
    tri_frequency[num, 3*(ae-1)+3] <- sum(unlist(tri_list[[ae]])==trigram)
    num <- num + 1
  }
}


par(mfrow=c(1, 3))
for(af in 1:length(tri_list)){
  tri_frequency <- tri_frequency[order(tri_frequency[,3*af],decreasing = TRUE),]
  barplot(head(tri_frequency[,3*af],5), names.arg = head(tri_frequency[,3*af-1],5),
          main=paste('Top 5 trigram frequency in', names[af]), cex.names = 1)
}
```


## Prediction Model
### Trigram(3-gram) model
### Katz's Backoff Model

## About Application